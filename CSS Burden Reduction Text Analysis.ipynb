{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\KyleEStreepy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\KyleEStreepy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KyleEStreepy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from scipy import stats\n",
    "import re\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "import warnings\n",
    "from sklearn.utils import resample\n",
    "import statistics\n",
    "import pyodbc\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    \n",
    "    def strip_html_tags(text):\n",
    "        soup = BeautifulSoup(text, \"html.parser\")\n",
    "        stripped_text = soup.get_text()\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_urls(text):\n",
    "        #url regex\n",
    "        url_re = re.compile(r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))\"\"\")\n",
    "        stripped_text = url_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_emails(text):\n",
    "        #email address regex\n",
    "        email_re = re.compile(r'(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+$)')\n",
    "        stripped_text = email_re.sub('',text)\n",
    "        return stripped_text\n",
    "\n",
    "    def strip_nonsense(text):\n",
    "        # leave words that are at least three characters long, do not contain a number, and are no more \n",
    "        # than 17 chars long\n",
    "        no_nonsense = re.findall(r'\\b[a-z][a-z][a-z]+\\b',text)\n",
    "        stripped_text = ' '.join(w for w in no_nonsense if w != 'nan' and len(w) <= 17)\n",
    "        return stripped_text\n",
    "    \n",
    "    doc = str(doc).lower()\n",
    "    tag_free = strip_html_tags(doc)\n",
    "    url_free = strip_urls(tag_free)\n",
    "    email_free = strip_emails(url_free)\n",
    "    normalized_1 = strip_nonsense(email_free)\n",
    "    \n",
    "    stop_free = \" \".join([i for i in normalized_1.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(WordNetLemmatizer().lemmatize(word) for word in punc_free.split())\n",
    "    \n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model(x):\n",
    "    \n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_components = 7\n",
    "    n_top_words = 10\n",
    "    \n",
    "    def print_top_words(model, feature_names, n_top_words):\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            message = \"Topic #%d: \" % topic_idx\n",
    "            message += \", \".join([feature_names[i]\n",
    "                                 for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "            print(message)\n",
    "        print()\n",
    "\n",
    "\n",
    "    print(\"Loading dataset...\")\n",
    "    t0 = time()\n",
    "    data_samples = x\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf-idf features for NMF.\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english',\n",
    "                                    ngram_range  = (1,2))\n",
    "    t0 = time()\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    # Use tf (raw term count) features for LDA.\n",
    "    print(\"Extracting tf features for LDA...\")\n",
    "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                   ngram_range = (1,2))\n",
    "    t0 = time()\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    t0 = time()\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "              alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "    # Fit the NMF model\n",
    "    print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "          \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    t0 = time()\n",
    "    nmf = NMF(n_components=n_components, random_state=1,\n",
    "              beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "              l1_ratio=.5).fit(tfidf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "    print(\"Fitting LDA models with tf features, \"\n",
    "          \"n_samples=%d and n_features=%d...\"\n",
    "          % (n_samples, n_features))\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    t0 = time()\n",
    "    lda.fit(tf)\n",
    "    print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "    print(\"\\nTopics in LDA model:\")\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(x):\n",
    "    raw_text = x.tolist()\n",
    "\n",
    "    text_data = []\n",
    "    for text in raw_text:\n",
    "        tokens = clean(text)\n",
    "        text_data.append(tokens)\n",
    "    \n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_to_list (x):\n",
    "    n_samples = 2000\n",
    "    n_features = 1000\n",
    "    n_components = 6\n",
    "    n_top_words = 10\n",
    "    #max_df=0.95, min_df=2,\n",
    "    tf_vectorizer = CountVectorizer(\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english',\n",
    "                                   ngram_range = (1,2))\n",
    "\n",
    "    tf = tf_vectorizer.fit_transform(x)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                    learning_method='online',\n",
    "                                    learning_offset=50.,\n",
    "                                    random_state=0)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    lda.fit(tf)\n",
    "    temp_list =[]\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        #message = \"Topic #%d: \" % topic_idx\n",
    "        message = ''\n",
    "        message += \", \".join([tf_feature_names[i]\n",
    "                            for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        temp_list.append(message)\n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agency \n",
    "Topic Modeling by Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br = pd.read_excel('data/CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br = df_br.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT', 'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br = df_br.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br['AGENCY'] = df_br['AGENCY'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_br[df_br['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br['AGENCY'] = df_br['AGENCY'].astype(str)\n",
    "df_br['low_value_text'] = df_br['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br =df_br.loc[df_br['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br['COMPOUND_SENT'] = df_br['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_agency = df_br['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agency in unique_agency:\n",
    "    temp_list = []\n",
    "    df_temp = df_br[df_br['AGENCY']==agency]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(agency)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(agency)\n",
    "        break\n",
    "    lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','agency','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='agency',value_vars = [x for x in list(df_lda_long.columns) if x !='agency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('Agency Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br.to_excel('Agency Sentiment_v2.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br_sent = df_br[['AGENCY','COMPOUND_SENT']].groupby('AGENCY').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br_sent.to_excel('Agency Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sr. Manager \n",
    "Topic Modeling by agency filtered to Supervisors Greater than GS-13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = pd.read_excel('data/CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.loc[df_mgr['SUP_STATUS'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT', 'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr = df_mgr.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['AGENCY'] = df_mgr['AGENCY'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_mgr[df_mgr['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['AGENCY'] = df_mgr['AGENCY'].astype(str)\n",
    "df_mgr['low_value_text'] = df_mgr['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr =df_mgr.loc[df_mgr['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr['COMPOUND_SENT'] = df_mgr['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_agency = df_mgr['AGENCY'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agency in unique_agency:\n",
    "    temp_list = []\n",
    "    df_temp = df_mgr[df_mgr['AGENCY']==agency]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(agency)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(agency)\n",
    "        break\n",
    "    lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','agency','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='agency',value_vars = [x for x in list(df_lda_long.columns) if x !='agency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('Sr. MGR Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr_sent = df_mgr[['AGENCY','COMPOUND_SENT']].groupby('AGENCY').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgr_sent.to_excel('Sr. MGR Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GS Groups\n",
    "Topic Modeling by GS Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = pd.read_excel('data/CSS_Burden_Reduction_2020_v1.xlsx', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gs_group(df):\n",
    "    gs_dict = {'GS-1' : 'GS 1-6',\n",
    "                'GS-2' : 'GS 1-6',\n",
    "                'GS-3' : 'GS 1-6',\n",
    "                'GS-4' : 'GS 1-6',\n",
    "                'GS-5' : 'GS 1-6',\n",
    "                'GS-6' : 'GS 1-6',\n",
    "                'GS-7' : 'GS 7-9',\n",
    "                'GS-8' : 'GS 7-9',\n",
    "                'GS-9' : 'GS 7-9',\n",
    "                'GS-10' : 'GS 10-12',\n",
    "                'GS-11' : 'GS 10-12',\n",
    "                'GS-12' : 'GS 10-12',\n",
    "                'GS-13' : 'GS 13',\n",
    "                'GS-14' : 'GS 14',\n",
    "                'GS-15' : 'GS 15',\n",
    "                'Other' : 'Other',\n",
    "                'SES' : 'SES',\n",
    "                'SL' : 'SL & ST',\n",
    "                'ST' : 'SL & ST',\n",
    "                'NULL': None}\n",
    "    \n",
    "    df['GS GROUP'] = df['GRADELEVEL'].map(gs_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = gs_group(df_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = df_gs.drop(['Select your Occupational Category', 'Where do you work?', 'Survey_ID',\n",
    "       'COMPONENT', 'SUB_COMPONENT', 'GRADELEVEL', 'SUP_STATUS', 'In your typical 40 hour week approximately how many hours would you classify as \"low value\" work?',\n",
    "       'How many years have you worked for the Federal Government?',\n",
    "       'Since you have worked for the Federal Government for less than 1 year, please specify the numbers of months you have worked for the Federal Government below.',\n",
    "       'How many years have you worked for your current Agency?',\n",
    "       'Since you have worked for your Agency for less than 1 year, please specify the number of months you have worked for your current agency below.',\n",
    "       'On average how many days per month do you telework?',\n",
    "       'What is the highest degree or level of education you have completed?',\n",
    "       'Please select your age.',\n",
    "       'Are you considering leaving your organization within the next year?'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs = df_gs.rename(index=str, columns={\n",
    "        'Please briefly describe an example of one burdensome administrative task or process which you believe is \"low value\"': 'low_value_text'\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df_gs[df_gs['low_value_text'].isnull()==False]['low_value_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs['GS GROUP'] = df_gs['GS GROUP'].astype(str)\n",
    "df_gs['low_value_text'] = df_gs['low_value_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs =df_gs.loc[df_gs['low_value_text'] != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs['COMPOUND_SENT'] = df_gs['low_value_text'].apply(lambda x: sid.polarity_scores(x)['compound'] if pd.isnull(x)==False else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "useless_words = ['would','could','should','le','non','federal','government','agency','way','low','value','work']\n",
    "exclude = set(string.punctuation) \n",
    "for word in useless_words:\n",
    "    stop.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_group = df_gs['GS GROUP'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in unique_group:\n",
    "    temp_list = []\n",
    "    df_temp = df_gs[df_gs['GS GROUP']==group]\n",
    "    sentences_temp = df_temp[df_temp['low_value_text'].isnull()==False]['low_value_text']\n",
    "    return_list = lda_to_list(tokenize_text(sentences_temp))\n",
    "    return_list.append(group)  \n",
    "    return_list.append(len(df_temp))\n",
    "    if return_list is None:\n",
    "        print(group)\n",
    "        break\n",
    "    lda_list.append(return_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list = lda_to_list(tokenize_text(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_list.append('ALL GOV')\n",
    "rand_list.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_list.append(rand_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_temp = ['topic_1','topic_2','topic_3','topic_4','topic_5','topic_6','gs group','comm_len']\n",
    "df_lda = pd.DataFrame(lda_list,columns =cols_temp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda['comm_len'] = pd.to_numeric(df_lda['comm_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long = df_lda[df_lda['comm_len']>25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_melt = pd.melt(df_lda_long,id_vars='gs group',value_vars = [x for x in list(df_lda_long.columns) if x !='gs group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lda_long.to_excel('GS Group Topics.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs_sent = df_gs[['GS GROUP','COMPOUND_SENT']].groupby('GS GROUP').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gs_sent.to_excel('GS Group Sentiment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
